import urllib.request
from urllib.parse import urlparse
import re #æ­£åˆ™
import os
from datetime import datetime, timedelta, timezone
import random
import opencc #ç®€ç¹è½¬æ¢

# æ‰§è¡Œå¼€å§‹æ—¶é—´
timestart = datetime.now()

#è¯»å–æ–‡æœ¬æ–¹æ³•
def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines]
            return lines
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

#read BlackList 2024-06-17 15:02
def read_blacklist_from_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    BlackList = [line.split(',')[1].strip() for line in lines if ',' in line]
    return BlackList

blacklist_auto=read_blacklist_from_txt('assets/whitelist-blacklist/blacklist_auto.txt') 
blacklist_manual=read_blacklist_from_txt('assets/whitelist-blacklist/blacklist_manual.txt') 
combined_blacklist = set(blacklist_auto + blacklist_manual)  #listæ˜¯ä¸ªåˆ—è¡¨ï¼Œsetæ˜¯ä¸ªé›†åˆï¼Œæ®è¯´æ£€ç´¢é€Ÿåº¦é›†åˆè¦å¿«å¾ˆå¤šã€‚2024-08-08

# å®šä¹‰å¤šä¸ªå¯¹è±¡ç”¨äºå­˜å‚¨ä¸åŒå†…å®¹çš„è¡Œæ–‡æœ¬
# ä¸»é¢‘é“
ys_lines = [] #å¤®è§†é¢‘é“
ws_lines = [] #å«è§†é¢‘é“
ty_lines = [] #ä½“è‚²é¢‘é“
gat_lines = [] #æ¸¯æ¾³å°
migu_lines = [] #å’ªå’•ç›´æ’­

# åœ°æ–¹å°
hb_lines = [] #åœ°æ–¹å°-æ¹–åŒ—é¢‘é“

other_lines = [] #å…¶ä»–
other_lines_url = [] # ä¸ºé™ä½otheræ–‡ä»¶å¤§å°ï¼Œå‰”é™¤é‡å¤urlæ·»åŠ 

whitelist_lines=read_txt_to_array('assets/whitelist-blacklist/whitelist_manual.txt') #ç™½åå•
whitelist_auto_lines=read_txt_to_array('assets/whitelist-blacklist/whitelist_auto.txt') #ç™½åå•

#è¯»å–æ–‡æœ¬
# ä¸»é¢‘é“
ys_dictionary=read_txt_to_array('ä¸»é¢‘é“/å¤®è§†é¢‘é“.txt')
ws_dictionary=read_txt_to_array('ä¸»é¢‘é“/å«è§†é¢‘é“.txt') 
ty_dictionary=read_txt_to_array('ä¸»é¢‘é“/ä½“è‚²é¢‘é“.txt') 
gat_dictionary=read_txt_to_array('ä¸»é¢‘é“/æ¸¯æ¾³å°.txt') 
migu_dictionary=read_txt_to_array('ä¸»é¢‘é“/å’ªå’•ç›´æ’­.txt') 

# åœ°æ–¹å°
hb_dictionary=read_txt_to_array('åœ°æ–¹å°/æ¹–åŒ—é¢‘é“.txt') 

# è‡ªå®šä¹‰æº
urls = read_txt_to_array('assets/urls.txt')

#ç®€ç¹è½¬æ¢
def traditional_to_simplified(text: str) -> str:
    # åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œ"t2s" è¡¨ç¤ºä»ç¹ä½“è½¬ä¸ºç®€ä½“
    converter = opencc.OpenCC('t2s')
    simplified_text = converter.convert(text)
    return simplified_text

#M3Uæ ¼å¼åˆ¤æ–­
def is_m3u_content(text):
    lines = text.splitlines()
    first_line = lines[0].strip()
    return first_line.startswith("#EXTM3U")

def convert_m3u_to_txt(m3u_content):
    # åˆ†è¡Œå¤„ç†
    lines = m3u_content.split('\n')
    
    # ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
    txt_lines = []
    
    # ä¸´æ—¶å˜é‡ç”¨äºå­˜å‚¨é¢‘é“åç§°
    channel_name = ""
    
    for line in lines:
        # è¿‡æ»¤æ‰ #EXTM3U å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç† #EXTINF å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°ï¼ˆå‡è®¾é¢‘é“åç§°åœ¨å¼•å·åï¼‰
            channel_name = line.split(',')[-1].strip()
        # å¤„ç† URL è¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p") :
            txt_lines.append(f"{channel_name},{line.strip()}")
        
        # å¤„ç†åç¼€åä¸ºm3uï¼Œä½†æ˜¯å†…å®¹ä¸ºtxtçš„æ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…é¢‘é“åç§°,URL çš„æ ¼å¼ï¼Œå¹¶ç¡®ä¿ URL åŒ…å« "://"
            # xxxx,http://xxxxx.xx.xx
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    # å°†ç»“æœåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”
    return '\n'.join(txt_lines)

# åœ¨listæ˜¯å¦å·²ç»å­˜åœ¨url 2024-07-22 11:18
def check_url_existence(data_list, url):
    """
    Check if a given URL exists in a list of data.

    :param data_list: List of strings containing the data
    :param url: The URL to check for existence
    :return: True if the URL exists in the list, otherwise False
    """
    if "127.0.0.1" in url:
        return False
    # Extract URLs from the data list
    urls = [item.split(',')[1] for item in data_list]
    return url not in urls #å¦‚æœä¸å­˜åœ¨åˆ™è¿”å›trueï¼Œéœ€è¦

# å¤„ç†å¸¦$çš„URLï¼ŒæŠŠ$ä¹‹åçš„å†…å®¹éƒ½å»æ‰ï¼ˆåŒ…æ‹¬$ä¹Ÿå»æ‰ï¼‰ ã€2024-08-08 22:29:11ã€‘
def clean_url(url):
    last_dollar_index = url.rfind('$')  # å®‰å…¨èµ·è§æ‰¾æœ€åä¸€ä¸ª$å¤„ç†
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# æ·»åŠ channel_nameå‰å‰”é™¤éƒ¨åˆ†ç‰¹å®šå­—ç¬¦
removal_list = ["ã€ŒIPV4ã€","ã€ŒIPV6ã€","[ipv6]","[ipv4]","_ç”µä¿¡", "ç”µä¿¡","ï¼ˆHDï¼‰","[è¶…æ¸…]","é«˜æ¸…","è¶…æ¸…", "-HD","(HK)","AKtv","@","IPV6","ğŸï¸","ğŸ¦"," ","[BD]","[VGA]","[HD]","[SD]","(1080p)","(720p)","(480p)"]
def clean_channel_name(channel_name, removal_list):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    channel_name = channel_name.replace("CCTV-", "CCTV");
    channel_name = channel_name.replace("CCTV0","CCTV");
    channel_name = channel_name.replace("PLUS", "+");
    channel_name = channel_name.replace("NewTV-", "NewTV");
    channel_name = channel_name.replace("iHOT-", "iHOT");
    channel_name = channel_name.replace("NEW", "New");
    channel_name = channel_name.replace("New_", "New");
    return channel_name

#è¯»å–çº é”™é¢‘é“åç§°æ–¹æ³•
def load_corrections_name(filename):
    corrections = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip(): #è·³è¿‡ç©ºè¡Œ
                continue
            parts = line.strip().split(',')
            correct_name = parts[0]
            for name in parts[1:]:
                corrections[name] = correct_name
    return corrections

#è¯»å–çº é”™æ–‡ä»¶
corrections_name = load_corrections_name('assets/corrections_name.txt')
def correct_name_data(name):
    if name in corrections_name and name != corrections_name[name]:
        name = corrections_name[name]
    return name
    
# åˆ†å‘ç›´æ’­æºï¼Œå½’ç±»ï¼ŒæŠŠè¿™éƒ¨åˆ†ä»process_urlå‰¥ç¦»å‡ºæ¥ï¼Œä¸ºä»¥ååŠ å…¥whitelistæºæ¸…å•åšå‡†å¤‡ã€‚
def process_channel_line(line):
    if  "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name = line.split(',')[0]
        channel_name = traditional_to_simplified(channel_name)  #ç¹è½¬ç®€
        channel_name = clean_channel_name(channel_name, removal_list)  #åˆ†å‘å‰æ¸…ç†channel_nameä¸­ç‰¹å®šå­—ç¬¦
        channel_name = correct_name_data(channel_name).strip() #æ ¹æ®çº é”™æ–‡ä»¶å¤„ç†
        
        channel_address = clean_url(line.split(',')[1]).strip()  #æŠŠURLä¸­$ä¹‹åçš„å†…å®¹éƒ½å»æ‰
        line=channel_name+","+channel_address #é‡æ–°ç»„ç»‡line
        
        if len(channel_address) > 0 and channel_address not in combined_blacklist: # åˆ¤æ–­å½“å‰æºæ˜¯å¦åœ¨blacklistä¸­
            # æ ¹æ®è¡Œå†…å®¹åˆ¤æ–­å­˜å…¥å“ªä¸ªå¯¹è±¡ï¼Œå¼€å§‹åˆ†å‘
            if channel_name in ys_dictionary: #å¤®è§†é¢‘é“
                if check_url_existence(ys_lines, channel_address):
                    ys_lines.append(line)
            elif channel_name in ws_dictionary: #å«è§†é¢‘é“
                if check_url_existence(ws_lines, channel_address):
                    ws_lines.append(line)
            elif channel_name in ty_dictionary: #ä½“è‚²é¢‘é“
                if check_url_existence(ty_lines, channel_address):
                    ty_lines.append(line)
            elif channel_name in gat_dictionary: #æ¸¯æ¾³å°
                if check_url_existence(gat_lines, channel_address):
                    gat_lines.append(line)
            elif channel_name in migu_dictionary: #å’ªå’•ç›´æ’­
                if check_url_existence(migu_lines, channel_address):
                    migu_lines.append(line)
            elif channel_name in hb_dictionary: #åœ°æ–¹å°-æ¹–åŒ—é¢‘é“
                if check_url_existence(hb_lines, channel_address):
                    hb_lines.append(line)
            else:
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)   #è®°å½•å·²åŠ url
                    other_lines.append(line)
                    
def process_url(url):
    print(f"å¤„ç†URL: {url}")
    try:
        other_lines.append(url+",#genre#")  # å­˜å…¥other_linesä¾¿äºcheck 2024-08-02 10:41
        
        # åˆ›å»ºä¸€ä¸ªè¯·æ±‚å¯¹è±¡å¹¶æ·»åŠ è‡ªå®šä¹‰header
        headers = {
            'User-Agent': 'PostmanRuntime-ApipostRuntime/1.1.0',
        }
        req = urllib.request.Request(url, headers=headers)
        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(req,timeout=10) as response:
            # ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–æ•°æ®
            data = response.read()
            # å°†äºŒè¿›åˆ¶æ•°æ®è§£ç ä¸ºå­—ç¬¦ä¸²
            try:
                # å…ˆå°è¯• UTF-8 è§£ç 
                text = data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    # è‹¥ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯• GBK è§£ç 
                    text = data.decode('gbk')
                except UnicodeDecodeError:
                    try:
                        # è‹¥ GBK è§£ç å¤±è´¥ï¼Œå°è¯• ISO-8859-1 è§£ç 
                        text = data.decode('iso-8859-1')
                    except UnicodeDecodeError:
                        print("æ— æ³•ç¡®å®šåˆé€‚çš„ç¼–ç æ ¼å¼è¿›è¡Œè§£ç ã€‚")
                        
            #å¤„ç†m3uæå–channel_nameå’Œchannel_address
            if is_m3u_content(text):
                text=convert_m3u_to_txt(text)

            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for line in lines:
                if  "#genre#" not in line and "," in line and "://" in line:
                    # æ‹†åˆ†æˆé¢‘é“åå’ŒURLéƒ¨åˆ†
                    channel_name, channel_address = line.split(',', 1)
                    #éœ€è¦åŠ å¤„ç†å¸¦#å·æº=äºˆåŠ é€Ÿæº
                    if "#" not in channel_address:
                        process_channel_line(line) # å¦‚æœæ²¡æœ‰äº•å·ï¼Œåˆ™ç…§å¸¸æŒ‰ç…§æ¯è¡Œè§„åˆ™è¿›è¡Œåˆ†å‘
                    else: 
                        # å¦‚æœæœ‰â€œ#â€å·ï¼Œåˆ™æ ¹æ®â€œ#â€å·åˆ†éš”
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline=f'{channel_name},{channel_url}'
                            process_channel_line(newline)

            other_lines.append('\n') #æ¯ä¸ªurlå¤„ç†å®Œæˆåï¼Œåœ¨other_linesåŠ ä¸ªå›è½¦ 2024-08-02 10:46

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def sort_data(order, data):
    # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯è¡Œæ•°æ®çš„ç´¢å¼•
    order_dict = {name: i for i, name in enumerate(order)}
    
    # å®šä¹‰ä¸€ä¸ªæ’åºé”®å‡½æ•°ï¼Œå¤„ç†ä¸åœ¨ order_dict ä¸­çš„å­—ç¬¦ä¸²
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))
    
    # æŒ‰ç…§ order ä¸­çš„é¡ºåºå¯¹æ•°æ®è¿›è¡Œæ’åº
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

#ç™½åå•åŠ å…¥
other_lines.append("ç™½åå•,#genre#")
print(f"æ·»åŠ ç™½åå• whitelist.txt")
for line in whitelist_lines:
    process_channel_line(line)

#è¯»å–whitelist,æŠŠé«˜å“åº”æºä»ç™½åå•ä¸­æŠ½å‡ºåŠ å…¥ã€‚
other_lines.append("ç™½åå•æµ‹é€Ÿ,#genre#")
print(f"æ·»åŠ ç™½åå• whitelist_auto.txt")
for line in whitelist_auto_lines:
    if  "#genre#" not in line and "," in line and "://" in line:
        parts = line.split(",")
        try:
            response_time = float(parts[0].replace("ms", ""))
        except ValueError:
            print(f"response_timeè½¬æ¢å¤±è´¥: {line}")
            response_time = 60000  # å•ä½æ¯«ç§’ï¼Œè½¬æ¢å¤±è´¥ç»™ä¸ª60ç§’
        if response_time < 2000: #2sä»¥å†…çš„é«˜å“åº”æº
            process_channel_line(",".join(parts[1:]))

#åŠ å…¥é…ç½®çš„url
for url in urls:
    if url.startswith("http"):        
        process_url(url)

# è·å–å½“å‰çš„ UTC æ—¶é—´
utc_time = datetime.now(timezone.utc)
# åŒ—äº¬æ—¶é—´
beijing_time = utc_time + timedelta(hours=8)
# æ ¼å¼åŒ–ä¸ºæ‰€éœ€çš„æ ¼å¼
formatted_time = beijing_time.strftime("%Y%m%d %H:%M")
version=formatted_time+",https://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8?contentid=2820180516001"

# ç˜¦èº«ç‰ˆ
all_lines_simple =  ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'] +\
                    ["å¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary,ys_lines) + ['\n'] + \
                    ["å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary,ws_lines) + ['\n'] + \
                    ["æ¸¯æ¾³å°,#genre#"] + sort_data(gat_dictionary,gat_lines) + ['\n'] + \
                    ["ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary,dy_lines) + ['\n'] + \
                    ["ç”µè§†å‰§é¢‘é“,#genre#"] + sort_data(dsj_dictionary,dsj_lines) + ['\n'] + \
                    ["ç»¼è‰ºé¢‘é“,#genre#"] + sort_data(zy_dictionary,zy_lines) + ['\n'] + \
                    ["NewTV,#genre#"] + sort_data(newtv_dictionary,newtv_lines) + ['\n'] + \
                    ["iHOT,#genre#"] + sort_data(ihot_dictionary,ihot_lines) + ['\n'] + \
                    ["ä½“è‚²é¢‘é“,#genre#"] + sort_data(ty_dictionary,ty_lines) + ['\n'] + \
                    ["å’ªå’•ç›´æ’­,#genre#"] + sort_data(migu_dictionary,migu_lines)+ ['\n'] + \
                    ["åŸ‹å †å †,#genre#"] + sort_data(mdd_dictionary,mdd_lines) + ['\n'] + \
                    ["éŸ³ä¹é¢‘é“,#genre#"] + sorted(yy_lines) + ['\n'] + \
                    ["æ¸¸æˆé¢‘é“,#genre#"] + sorted(game_lines) + ['\n'] + \
                    ["è§£è¯´é¢‘é“,#genre#"] + sorted(js_lines)

# åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬ï¼ˆå»é‡ï¼Œæ’åºåæ‹¼æ¥ï¼‰
all_lines =  all_lines_simple + ['\n'] + \
             ["å„¿ç«¥,#genre#"] + sort_data(et_dictionary,et_lines) + ['\n'] + \
             ["å›½é™…å°,#genre#"] + sort_data(gj_dictionary,gj_lines) + ['\n'] + \
             ["çºªå½•ç‰‡,#genre#"] + sort_data(jlp_dictionary,jlp_lines)+ ['\n'] + \
             ["æˆæ›²é¢‘é“,#genre#"] + sort_data(xq_dictionary,xq_lines) + ['\n'] + \
             ["ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(sh_dictionary,sh_lines) + ['\n'] + \
             ["æ¹–å—é¢‘é“,#genre#"] + sort_data(hn_dictionary,hn_lines) + ['\n'] + \
             ["æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(hb_dictionary,hb_lines) + ['\n'] + \
             ["å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(gd_dictionary,gd_lines) + ['\n'] + \
             ["æµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(zj_dictionary,zj_lines) + ['\n'] + \
             ["å±±ä¸œé¢‘é“,#genre#"] + sort_data(shandong_dictionary,shandong_lines) + ['\n'] + \
             ["æ±Ÿè‹é¢‘é“,#genre#"] + sorted(jsu_lines) + ['\n'] + \
             ["å®‰å¾½é¢‘é“,#genre#"] + sorted(ah_lines) + ['\n'] + \
             ["æµ·å—é¢‘é“,#genre#"] + sorted(hain_lines) + ['\n'] + \
             ["å†…è’™é¢‘é“,#genre#"] + sorted(nm_lines) + ['\n'] + \
             ["è¾½å®é¢‘é“,#genre#"] + sorted(ln_lines) + ['\n'] + \
             ["é™•è¥¿é¢‘é“,#genre#"] + sorted(sx_lines) + ['\n'] + \
             ["å±±è¥¿é¢‘é“,#genre#"] + sorted(shanxi_lines) + ['\n'] + \
             ["äº‘å—é¢‘é“,#genre#"] + sorted(yunnan_lines) + ['\n'] + \
             ["åŒ—äº¬é¢‘é“,#genre#"] + sorted(bj_lines) + ['\n'] + \
             ["é‡åº†é¢‘é“,#genre#"] + sorted(cq_lines) + ['\n'] + \
             ["ç¦å»ºé¢‘é“,#genre#"] + sorted(fj_lines) + ['\n'] + \
             ["ç”˜è‚ƒé¢‘é“,#genre#"] + sorted(gs_lines) + ['\n'] + \
             ["å¹¿è¥¿é¢‘é“,#genre#"] + sorted(gx_lines) + ['\n'] + \
             ["è´µå·é¢‘é“,#genre#"] + sorted(gz_lines) + ['\n'] + \
             ["æ²³åŒ—é¢‘é“,#genre#"] + sorted(heb_lines) + ['\n'] + \
             ["æ²³å—é¢‘é“,#genre#"] + sorted(hen_lines) + ['\n'] + \
             ["é»‘é¾™æ±Ÿé¢‘é“,#genre#"] + sorted(hlj_lines) + ['\n'] + \
             ["å‰æ—é¢‘é“,#genre#"] + sorted(jl_lines) + ['\n'] + \
             ["æ±Ÿè¥¿é¢‘é“,#genre#"] + sorted(jx_lines) + ['\n'] + \
             ["å®å¤é¢‘é“,#genre#"] + sorted(nx_lines) + ['\n'] + \
             ["é’æµ·é¢‘é“,#genre#"] + sorted(qh_lines) + ['\n'] + \
             ["å››å·é¢‘é“,#genre#"] + sorted(sc_lines) + ['\n'] + \
             ["å¤©æ´¥é¢‘é“,#genre#"] + sorted(tj_lines) + ['\n'] + \
             ["æ–°ç–†é¢‘é“,#genre#"] + sorted(xj_lines) + ['\n'] + \
             ["æ˜¥æ™š,#genre#"] + sort_data(cw_dictionary,cw_lines)  + ['\n'] + \
             ["ç›´æ’­ä¸­å›½,#genre#"] + sorted(zb_lines) + ['\n'] + \
             ["MTV,#genre#"] + sorted(mtv_lines) + ['\n'] + \
             ["æ”¶éŸ³æœºé¢‘é“,#genre#"] + sort_data(radio_dictionary,radio_lines)

# å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
output_file = "live.txt"
output_file_simple = "live_lite.txt"
# æœªåŒ¹é…çš„å†™å…¥æ–‡ä»¶
others_file = "others.txt"

try:
    # ç˜¦èº«ç‰ˆ
    with open(output_file_simple, 'w', encoding='utf-8') as f:
        for line in all_lines_simple:
            f.write(line + '\n')
    print(f"åˆå¹¶åçš„ç²¾ç®€æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file_simple}")

    # å…¨é›†ç‰ˆ
    with open(output_file, 'w', encoding='utf-8') as f:
        for line in all_lines:
            f.write(line + '\n')
    print(f"åˆå¹¶åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")

    # å…¶ä»–
    with open(others_file, 'w', encoding='utf-8') as f:
        for line in other_lines:
            f.write(line + '\n')
    print(f"å…¶ä»–å·²ä¿å­˜åˆ°æ–‡ä»¶: {others_file}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def make_m3u(txt_file, m3u_file):
    try:
        output_text = '#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"\n'
        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()

        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url="https://epg.112114.xyz/logo/"+channel_name+".png"
                if logo_url is None: #not found logo
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"

        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)
        print(f"M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

make_m3u(output_file, "live.m3u")
make_m3u(output_file_simple, "live_lite.m3u")

# æ‰§è¡Œç»“æŸæ—¶é—´
timeend = datetime.now()

# è®¡ç®—æ—¶é—´å·®
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()

# è½¬æ¢ä¸ºåˆ†é’Ÿå’Œç§’
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)

print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")

combined_blacklist_hj = len(combined_blacklist)
all_lines_hj = len(all_lines)
other_lines_hj = len(other_lines)
print(f"blacklistè¡Œæ•°: {combined_blacklist_hj} ")
print(f"live.txtè¡Œæ•°: {all_lines_hj} ")
print(f"others.txtè¡Œæ•°: {other_lines_hj} ")

#å¤‡ç”¨1ï¼šhttp://tonkiang.us
#å¤‡ç”¨2ï¼šhttps://www.zoomeye.hk,https://www.shodan.io,https://tv.cctv.com/live/
#å¤‡ç”¨3ï¼š(BlackListæ£€æµ‹å¯¹è±¡)http,rtmp,p3p,rtpï¼ˆrtspï¼Œp2pï¼‰
